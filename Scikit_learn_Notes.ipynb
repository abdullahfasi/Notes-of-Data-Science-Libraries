{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit-learn **is a widely-used Python library for machine learning that provides simple and efficient tools for data mining and analysis. It is built on top of NumPy, SciPy, and matplotlib and offers various algorithms and utilities for both supervised and unsupervised learning."
      ],
      "metadata": {
        "id": "TfYToOHKmIBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6zkgG8Gl29m"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets\n",
        "\n",
        "Scikit-learn provides several small sample datasets (like the famous Iris dataset) and utilities to generate or load custom datasets."
      ],
      "metadata": {
        "id": "XaxlhfW7p_34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Features and target\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(X.shape)  # (150, 4)\n",
        "print(y.shape)  # (150,)"
      ],
      "metadata": {
        "id": "o93dvaXrqDWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Iris dataset contains 150 samples of iris flowers, each described by four features (sepal length, sepal width, petal length, petal width), and the target is the type of flower (three species)."
      ],
      "metadata": {
        "id": "GD2NcTuuqJGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #Train-Test Split\n",
        "\n",
        "In machine learning, it's essential to split the data into training and testing sets to evaluate model performance on unseen data."
      ],
      "metadata": {
        "id": "VwXCHIcfqSty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, X_test.shape)  # (120, 4), (30, 4)"
      ],
      "metadata": {
        "id": "Zdd2tT9nqWBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Preprocessing\n",
        "\n",
        "Scikit-learn includes various utilities for preprocessing data, such as scaling features, normalizing, or encoding categorical variables.\n",
        "\n",
        "Standardization (scaling): This process scales the features so that they have zero mean and unit variance.\n",
        "\n",
        "Scikit-learn offers a rich collection of utilities for preprocessing data, which is a crucial step in machine learning to ensure data quality and consistency. These preprocessing techniques help to improve model performance and prevent issues such as overfitting.\n",
        "\n",
        "#Example:"
      ],
      "metadata": {
        "id": "tU5C4SvZrUBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled[:5])  # First 5 standardized samples"
      ],
      "metadata": {
        "id": "JXPYiiuQrXzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler() scales the features to have a mean of 0 and standard deviation of 1. This is important for many machine learning models that are sensitive to the scale of input features (e.g., SVM, k-NN).\n",
        "\n"
      ],
      "metadata": {
        "id": "zqQM3BHwsCfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Learning Algorithms\n",
        "\n",
        "Scikit-learn provides many algorithms for supervised learning, which is the task of learning a function from labeled training data."
      ],
      "metadata": {
        "id": "6MLDdQPjsvhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train a logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "8IWUdVVcs8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticRegression() is used for classification, and the accuracy_score() function evaluates how well the model performed on the test set."
      ],
      "metadata": {
        "id": "yiVL7nG-s-l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised Learning Algorithms\n",
        "\n",
        "Unsupervised learning involves learning patterns from unlabeled data. Scikit-learn provides tools for clustering, dimensionality reduction, and more."
      ],
      "metadata": {
        "id": "ypKJwhP-tRjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of K-Means Clustering:\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "Here, K-Means is used to cluster the data into 3 groups. Clustering is unsupervised, meaning we donâ€™t need target labels."
      ],
      "metadata": {
        "id": "tD4Z5FrutilR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Cluster centers and labels\n",
        "print(\"Cluster centers:\\n\", kmeans.cluster_centers_)\n",
        "print(\"Labels:\\n\", kmeans.labels_)"
      ],
      "metadata": {
        "id": "jdAl_MDdtnkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation\n",
        "\n",
        "Scikit-learn provides several ways to evaluate a model's performance, including metrics for classification and regression.\n",
        "\n",
        "Accuracy, Precision, Recall, F1-Score\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "Mean Squared Error (MSE)\n",
        "\n",
        "Example of Confusion Matrix and Classification Report:"
      ],
      "metadata": {
        "id": "kcnMM8NWu9Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "SOmYQG5-vBK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix gives insight into how many true positives, false positives, etc., are made, while the classification report provides metrics such as precision, recall, and F1-score.\n",
        "\n",
        "Confusion Matrix:\n",
        "\n",
        "The diagonal elements indicate correct classifications. For example, cm[0, 0] represents the number of true positives (correctly predicted class 0).\n",
        "The off-diagonal elements indicate misclassifications. For example, cm[0, 1] represents the number of false negatives (class 0 instances incorrectly predicted as class 1)."
      ],
      "metadata": {
        "id": "M_NBH0IiwFV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation\n",
        "\n",
        "Cross-validation is a technique to assess the performance of a model on different subsets of data. The most common form is k-fold cross-validation, which divides the dataset into k folds and trains the model k times."
      ],
      "metadata": {
        "id": "MOBm8f8dvEWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
        "\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average CV score:\", cv_scores.mean())"
      ],
      "metadata": {
        "id": "cQVg_oXxvG10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelines\n",
        "\n",
        "A Pipeline is a tool that allows you to chain multiple steps together (e.g., preprocessing + model) so that you can fit and evaluate everything in one go. This is useful to ensure that preprocessing (like scaling) is done only on the training data during cross-validation or testing."
      ],
      "metadata": {
        "id": "SF3OwmNbvJ9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define a pipeline with scaling and model training\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Train the model using the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the pipeline\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "e4NCvViWvM2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Grid Search for Hyperparameter Tuning\n",
        "\n",
        "Grid Search helps in finding the best hyperparameters for a model by exhaustively searching over a specified parameter grid."
      ],
      "metadata": {
        "id": "HOQmeVA8vPOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear']}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "XMuCJZmmvSo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dimensionality Reduction\n",
        "\n",
        "Scikit-learn provides dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce the number of features while preserving variance."
      ],
      "metadata": {
        "id": "a7mUKMvMvVCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce the data to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "print(\"Reduced data shape:\", X_reduced.shape)"
      ],
      "metadata": {
        "id": "a2ShcrfNvYlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}