{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tensors\n",
        "\n",
        "Tensors are the fundamental data structures in PyTorch, similar to NumPy arrays but with additional capabilities such as GPU acceleration. PyTorch tensors can be used for building neural networks, performing operations, and holding data in models."
      ],
      "metadata": {
        "id": "Btf710j622-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHUsVpbvywWB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor (2x3 matrix)\n",
        "tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "\n",
        "print(tensor)\n",
        "print(tensor.shape)  # Output: torch.Size([2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch allows you to perform various tensor operations like addition, multiplication, and matrix operations similar to NumPy, but with the added benefit of GPU support."
      ],
      "metadata": {
        "id": "yeUv_wdY3z1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic tensor operations\n",
        "a = torch.tensor([1.0, 2.0])\n",
        "b = torch.tensor([3.0, 4.0])\n",
        "\n",
        "# Element-wise addition\n",
        "c = a + b\n",
        "\n",
        "# Matrix multiplication\n",
        "d = torch.matmul(a.unsqueeze(0), b.unsqueeze(1))\n",
        "\n",
        "print(c)  # Output: tensor([4., 6.])\n",
        "print(d)  # Output: tensor([[11.]])"
      ],
      "metadata": {
        "id": "PEjMCd-w30Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd (Automatic Differentiation)\n",
        "\n",
        "One of PyTorch's most powerful features is autograd, which allows for automatic computation of gradients for tensors. This is crucial for training neural networks because gradients are used to update model weights."
      ],
      "metadata": {
        "id": "E7rbrnkJ4yhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tensor with requires_grad=True to track operations for automatic differentiation\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Perform some operations\n",
        "y = x ** 2 + 3 * x\n",
        "\n",
        "# Compute gradients by calling backward()\n",
        "y.backward()\n",
        "\n",
        "# The gradient of y with respect to x (dy/dx)\n",
        "print(x.grad)  # Output: tensor(7.)"
      ],
      "metadata": {
        "id": "k3gJNkDv47Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, requires_grad=True tells PyTorch to keep track of all operations on x so that when backward() is called, it can compute the derivative (gradient)."
      ],
      "metadata": {
        "id": "N7O8LpPC4_0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Computation Graph\n",
        "\n",
        "PyTorch uses a dynamic computation graph, meaning the graph is created on-the-fly when operations are performed. This makes debugging and experimenting easier since the graph is generated dynamically with each forward pass."
      ],
      "metadata": {
        "id": "namlsxvV5_kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "    return x ** 2 + 3 * x + 5\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = forward(x)\n",
        "y.backward()\n",
        "\n",
        "print(x.grad)  # Gradient will still be 9, but the graph is created dynamically"
      ],
      "metadata": {
        "id": "Ais-dUqo6B54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike frameworks like TensorFlow (1.x), where the computation graph is static and defined before running, PyTorch builds the graph dynamically as the operations are executed."
      ],
      "metadata": {
        "id": "jEFq5Egh6EDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks (nn.Module)\n",
        "\n",
        "In PyTorch, neural networks are built using the torch.nn module, which contains many pre-built layers like fully connected layers, convolutional layers, and more. A neural network in PyTorch is typically defined as a class that inherits from torch.nn.Module."
      ],
      "metadata": {
        "id": "SVp9CcJg6H_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple feedforward neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(4, 10)  # Input layer (4 features), output (10 neurons)\n",
        "        self.fc2 = nn.Linear(10, 3)  # Hidden layer (10 neurons), output (3 neurons)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# Instantiate the network\n",
        "net = SimpleNet()\n",
        "\n",
        "# Example input\n",
        "input_data = torch.rand(1, 4)  # 1 sample with 4 features\n",
        "output = net(input_data)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "hiVntXrm6FzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines a neural network with two fully connected (linear) layers. The forward() function specifies how data flows through the network, and torch.relu() is an activation function applied to the output of the first layer."
      ],
      "metadata": {
        "id": "_ng-ABg46O0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions\n",
        "\n",
        "Loss functions in PyTorch measure how well or poorly a model's predictions match the true labels. Common loss functions include:\n",
        "\n",
        "Mean Squared Error (MSE): Used for regression tasks.\n",
        "Cross-Entropy Loss: Used for classification tasks."
      ],
      "metadata": {
        "id": "ILL6QNbn6Q5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Dummy target and prediction\n",
        "pred = torch.tensor([[1.0, 2.0, 3.0]])\n",
        "target = torch.tensor([2])\n",
        "\n",
        "# Calculate loss\n",
        "loss = loss_fn(pred, target)\n",
        "print(loss)  # Cross-entropy loss"
      ],
      "metadata": {
        "id": "roXwoNBN6TAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nn.CrossEntropyLoss() combines LogSoftmax and NLLLoss in one class, which is typically used for multi-class classification problems."
      ],
      "metadata": {
        "id": "cTMcQISS6UnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization\n",
        "\n",
        "Optimization algorithms are used to adjust the model parameters (weights) to minimize the loss function. PyTorch provides several optimization algorithms in torch.optim, such as SGD, Adam, and RMSProp."
      ],
      "metadata": {
        "id": "unLKpjAK6yIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training step\n",
        "optimizer.zero_grad()  # Zero out gradients before backpropagation\n",
        "loss.backward()  # Compute gradients\n",
        "optimizer.step()  # Update model weights"
      ],
      "metadata": {
        "id": "RA8tUcb06Ykx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the optimizer adjusts the parameters of the neural network based on the computed gradients to minimize the loss function. optimizer.zero_grad() is called to clear the gradients before computing them again in the next iteration."
      ],
      "metadata": {
        "id": "VDV5PUSU6aWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Model\n",
        "\n",
        "Putting everything together, training a model in PyTorch involves forward passing the data through the network, computing the loss, backpropagating the gradients, and updating the weights."
      ],
      "metadata": {
        "id": "k3HD4oAp6cip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple dataset (dummy data)\n",
        "X = torch.rand(100, 4)  # 100 samples with 4 features\n",
        "y = torch.randint(0, 3, (100,))  # 100 labels, 3 classes\n",
        "\n",
        "# Define network, loss function, and optimizer\n",
        "net = SimpleNet()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # 100 epochs\n",
        "    optimizer.zero_grad()  # Zero out gradients\n",
        "    output = net(X)  # Forward pass\n",
        "    loss = loss_fn(output, y)  # Compute loss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "06S41-Qi67mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple example of a training loop. The data is passed through the network, the loss is computed, and the weights are updated iteratively. Every 10 epochs, the loss is printed to track training progress."
      ],
      "metadata": {
        "id": "9IZdcqYq697y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets and DataLoaders\n",
        "\n",
        "PyTorch provides the torch.utils.data module, which includes Dataset and DataLoader classes for loading and batching data, which is especially useful for large datasets.\n",
        "\n",
        "Dataset: Defines how to access the data.\n",
        "DataLoader: Loads data in batches for training."
      ],
      "metadata": {
        "id": "Dh8fZmQC6_tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CustomDataset(X, y)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Iterate through data in batches\n",
        "for batch_data, batch_labels in dataloader:\n",
        "    print(batch_data, batch_labels)\n"
      ],
      "metadata": {
        "id": "Tnaa97We7B_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DataLoader class automatically handles batching, shuffling, and parallel loading, making it easy to work with large datasets during training."
      ],
      "metadata": {
        "id": "UH5RmhbX7EOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " GPU Acceleration (CUDA)\n",
        "\n",
        "One of PyTorchâ€™s key strengths is its seamless support for GPU acceleration using CUDA. By sending tensors and models to the GPU, operations can be performed much faster."
      ],
      "metadata": {
        "id": "3NJuNF9y7F_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move tensor to GPU\n",
        "tensor = torch.tensor([[1.0, 2.0, 3.0]], device=device)\n",
        "\n",
        "# Move model to GPU\n",
        "net = SimpleNet().to(device)\n",
        "\n",
        "# Move input to GPU before feeding it to the model\n",
        "output = net(tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "oeBEAOjU7LMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With tensor.to(device) and model.to(device), you can move both data and models to the GPU, where operations can be performed much faster."
      ],
      "metadata": {
        "id": "Ms94uMxK7Ndw"
      }
    }
  ]
}